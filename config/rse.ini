[transformer]
d_model = 32
n_encoders = 1
n_heads = 2
d_attn = 16
d_embed = 9
use_pre = 0

[training]
batch_size = 2
n_epochs = 5
val_size = 4
val_step = 100
lr = 0.00001
grad_clip = 1
